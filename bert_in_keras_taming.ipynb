{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "\n",
    "We have already seen many BERT model in raw tensorflow and pytorch. \n",
    "As keras is with user-friendly UI and easy to use, I want to find out whether there is BERT model in keras.\n",
    "Finally, https://github.com/CyberZHG/keras-bert is what I need. I have packed it into my database (https://www.kaggle.com/httpwwwfszyc/kerasbert).\n",
    "\n",
    "Here I just redo the thing similar to what taindow did in keras bert(https://www.kaggle.com/taindow/bert-a-fine-tuning-example)\n",
    "* ~~No data prepocessing~~ and no warm up\n",
    "* We only use 1/3 of the training data\n",
    "* We use a maximum sequence length of 72\n",
    "\n",
    "Similarly, thanks for Jon Mischo (https://www.kaggle.com/supertaz) for uploading BERT Models + Scripts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import sys\n",
    "import random\n",
    "import keras\n",
    "import tensorflow as tf\n",
    "import json\n",
    "#sys.path.insert(0, '../input/pretrained-bert-including-scripts/master/bert-master')\n",
    "#!cp -r '../input/kerasbert/keras_bert' '/kaggle/working'\n",
    "BERT_PRETRAINED_DIR = '../input/pretrained-bert-including-scripts/uncased_l-12_h-768_a-12/uncased_L-12_H-768_A-12'\n",
    "print('***** BERT pretrained directory: {} *****'.format(BERT_PRETRAINED_DIR))\n",
    "import tokenization  #Actually keras_bert contains tokenization part, here just for convenience"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load raw model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras_bert.keras_bert.loader import load_trained_model_from_checkpoint\n",
    "from keras.optimizers import Adam\n",
    "adam = Adam(lr=2e-5,decay=0.01)\n",
    "maxlen = 72\n",
    "print('begin_build')\n",
    "\n",
    "config_file = os.path.join(BERT_PRETRAINED_DIR, 'bert_config.json')\n",
    "checkpoint_file = os.path.join(BERT_PRETRAINED_DIR, 'bert_model.ckpt')\n",
    "model = load_trained_model_from_checkpoint(config_file, checkpoint_file, training=True,seq_len=maxlen)\n",
    "#model.summary(line_length=120)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build classification model\n",
    "\n",
    "As the Extract layer extracts only the first token where \"['CLS']\" used to be, we just take the layer and connect to the single neuron output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.layers import Dense,Input,Flatten,concatenate,Dropout,Lambda\n",
    "from keras.models import Model\n",
    "import keras.backend as K\n",
    "import re\n",
    "import codecs\n",
    "\n",
    "sequence_output  = model.layers[-6].output\n",
    "pool_output = Dense(1, activation='sigmoid',kernel_initializer=keras.initializers.TruncatedNormal(stddev=0.02),name = 'real_output')(sequence_output)\n",
    "model3  = Model(inputs=model.input, outputs=pool_output)\n",
    "model3.compile(loss='binary_crossentropy', optimizer=adam)\n",
    "model3.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare Data, Training, Predicting\n",
    "\n",
    "First the model need train data like [token_input,seg_input,masked input], here we set all segment input to 0 and all masked input to 1.\n",
    "\n",
    "Still I am finding a more efficient way to do token-convert-to-ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_lines(example, max_seq_length,tokenizer):\n",
    "    max_seq_length -=2\n",
    "    all_tokens = []\n",
    "    longer = 0\n",
    "    for i in range(example.shape[0]):\n",
    "      tokens_a = tokenizer.tokenize(example[i])\n",
    "      if len(tokens_a)>max_seq_length:\n",
    "        tokens_a = tokens_a[:max_seq_length]\n",
    "        longer += 1\n",
    "      one_token = tokenizer.convert_tokens_to_ids([\"[CLS]\"]+tokens_a+[\"[SEP]\"])+[0] * (max_seq_length - len(tokens_a))\n",
    "      all_tokens.append(one_token)\n",
    "    print(longer)\n",
    "    return np.array(all_tokens)\n",
    "    \n",
    "nb_epochs=1\n",
    "bsz = 32\n",
    "dict_path = os.path.join(BERT_PRETRAINED_DIR, 'vocab.txt')\n",
    "tokenizer = tokenization.FullTokenizer(vocab_file=dict_path, do_lower_case=True)\n",
    "print('build tokenizer done')\n",
    "train_df = pd.read_csv('../input/jigsaw-unintended-bias-in-toxicity-classification/train.csv')\n",
    "train_df = train_df.sample(frac=0.33,random_state = 42)\n",
    "train_df['comment_text'] = train_df['comment_text'].replace({r'\\s+$': '', r'^\\s+': ''}, regex=True).replace(r'\\n',  ' ', regex=True)\n",
    "\n",
    "train_lines, train_labels = train_df['comment_text'].values, train_df.target.values \n",
    "print('sample used',train_lines.shape)\n",
    "token_input = convert_lines(train_lines,maxlen,tokenizer)\n",
    "seg_input = np.zeros((token_input.shape[0],maxlen))\n",
    "mask_input = np.ones((token_input.shape[0],maxlen))\n",
    "print(token_input.shape)\n",
    "print(seg_input.shape)\n",
    "print(mask_input.shape)\n",
    "print('begin training')\n",
    "model3.fit([token_input, seg_input, mask_input],train_labels,batch_size=bsz,epochs=nb_epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load test data\n",
    "test_df = pd.read_csv('../input/jigsaw-unintended-bias-in-toxicity-classification/test.csv')\n",
    "test_df['comment_text'] = test_df['comment_text'].replace({r'\\s+$': '', r'^\\s+': ''}, regex=True).replace(r'\\n',  ' ', regex=True)\n",
    "eval_lines = test_df['comment_text'].values\n",
    "print(eval_lines.shape)\n",
    "print('load data done')\n",
    "token_input2 = convert_lines(eval_lines,maxlen,tokenizer)\n",
    "seg_input2 = np.zeros((token_input2.shape[0],maxlen))\n",
    "mask_input2 = np.ones((token_input2.shape[0],maxlen))\n",
    "print('test data done')\n",
    "print(token_input2.shape)\n",
    "print(seg_input2.shape)\n",
    "print(mask_input2.shape)\n",
    "hehe = model3.predict([token_input2, seg_input2, mask_input2],verbose=1,batch_size=bsz)\n",
    "submission = pd.read_csv('../input/jigsaw-unintended-bias-in-toxicity-classification/sample_submission.csv', index_col='id')\n",
    "submission['prediction'] = hehe\n",
    "submission.reset_index(drop=False, inplace=True)\n",
    "submission.to_csv('submission.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
